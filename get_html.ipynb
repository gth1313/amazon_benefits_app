{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a directory to save the HTML files\n",
    "directory = \"amazon-docs\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Base URL of the website\n",
    "base_url = \"https://www.aboutamazon.com/news/workplace/amazons-20-weeks-of-paid-leave-helped-this-new-mom-find-balance\"\n",
    "\n",
    "# Function to download and save HTML content\n",
    "def save_html(url, path):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(path, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve {url}\")\n",
    "\n",
    "# Function to recursively download and save linked pages\n",
    "def download_linked_pages(url, visited, depth=0):\n",
    "    if depth > 5:  # Adjust the depth limit as needed\n",
    "        return\n",
    "    if url in visited:\n",
    "        return\n",
    "    visited.add(url)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        filename = urlparse(url).path.replace('/', '_') + '.html'\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        save_html(url, file_path)\n",
    "        \n",
    "        # Find all links on the page\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            # Skip external links\n",
    "            if urlparse(href).netloc and urlparse(href).netloc != urlparse(base_url).netloc:\n",
    "                continue\n",
    "            # Convert relative URLs to absolute URLs\n",
    "            file_url = urljoin(base_url, href)\n",
    "            download_linked_pages(file_url, visited, depth + 1)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve {url}\")\n",
    "\n",
    "# Start the recursive download from the main page\n",
    "visited_urls = set()\n",
    "download_linked_pages(base_url, visited_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
